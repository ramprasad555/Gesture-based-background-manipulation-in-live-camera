
**Group name - Techno Turtles**

**Group Members** - Pavan Kumar Jonnadula(pj1098), Ramprasad Kokkula (rk1668), Aakanksha Padmanabhan(al7275), Vaishnavi Patil (vp2156)

**Project Title - Gesture Recognition and Background Manipulation in Live Camera.**

**Abstract / Overview**

In the realm of computer vision and human-computer interaction, our project focuses on Gesture Recognition and Background Manipulation in Live Camera. Recognizing the significance of visual communication today, our goal is to lead in real-time gesture recognition and background manipulation, opening doors to applications like enriched gaming and improved video conferencing.

Our project introduces an innovative real-time gesture-controlled system to effortlessly tweak backgrounds in live videos and images. The dynamic backgrounds act as an interactive canvas, achieved through customizing existing gesture recognition models. However, challenges exist, such as user engagement limitations, generic gesture models, background manipulation shortcomings, transition issues, and complex gesture-background correlations.

Our project tackles these challenges head-on, aiming to create a solution that seamlessly merges real-time gesture recognition with dynamic background changes. By doing so, we aim to boost user engagement and offer a tailored, interactive experience in dynamic background manipulation for both live videos and images. This project strives to redefine visual interactions, providing an immersive platform that caters to diverse user preferences and environments.


**Learning Objectives**

In our project, we want to explore the concepts of computer vision, focusing on image processing, feature extraction, and recognizing objects. Our main focus is on understanding gestures and changing backgrounds in images. We'll dive into deep learning, especially Convolutional Neural Networks (CNNs), to become skilled at training and using models for accurate gesture recognition.

During the project, our main goal is to learn about hand recognition models and how they work. We'll actively contribute to making a special dataset designed for training our chosen model. Using established CNNs, we'll understand how to train the model with our dataset. We'll then use this knowledge in real-life situations, applying techniques to separate the hand from the background. This hands-on approach will give us practical experience, from training the model to using it in the real world, making us more skilled in hand recognition and image processing.






**Discussion of ethics**

We'll make sure to consider ethical concerns at every stage of our project. The following key ethical considerations will guide our approach:

**1. Privacy Protection:**

To safeguard user privacy, all collected data, whether for model training or system evaluation, will be anonymized and stored securely. 

**2. Bias Mitigation:**

Regular evaluations will be conducted to ensure fair and unbiased outcomes for all users, irrespective of individual characteristics.

**3. Informed Consent:**

The purpose of data collection, its use, and participants' rights will be clearly communicated, emphasizing a commitment to ethical data handling practices.

**4. Ongoing Evaluation and Monitoring:**

Continuous evaluation and monitoring will be integral to our ethical framework. Regular assessments will be conducted to identify and rectify biases, incorporating user feedback mechanisms to adapt the system to evolving ethical considerations and user needs.

**5. Documentation and Accountability:**

Transparent documentation of ethical considerations, decision-making processes, and accountability measures will be maintained throughout the project. This commitment ensures a clear record of ethical practices and facilitates accountability for the responsible development and deployment of our system.


**What Exists and how we’ll use it**

Computer Vision Libraries:

**OpenCV:** We will use OpenCV for various image processing tasks, including background manipulation and segmentation. Its rich set of functions makes it a valuable tool for our computer vision needs.

Machine Learning Frameworks:

**TensorFlow and PyTorch:** These frameworks are essential for implementing and training Convolutional Neural Networks (CNNs) for gesture recognition. Leveraging pre-trained models and fine-tuning them to our specific requirements will expedite our progress.

Gesture Recognition Models:

**MediaPipe Hands:** We plan to explore and potentially use the MediaPipe Hands library for hand gesture recognition. This existing model can serve as a strong foundation, and we may customize it to align with our project's objectives.


**Reach Goal**

This project aims to smoothly combine recognizing gestures with changing backgrounds, providing users with an engaging and interactive experience.

**Minimum Goal**

Our project currently has two minimal goals both of which can be achieved separately. 

1. **Segmentation** 

This involves developing a system capable of accurately identifying and isolating relevant elements within an image, laying the foundation for subsequent gesture recognition and background manipulation. 

1. **Gesture Detection**

The system aims to recognize and interpret at least two distinct gestures from users interacting with the static image. This phase establishes the groundwork for the overarching goal of dynamic background manipulation through intuitive user gestures. 

**Milestones and internal deadlines**

|**Milestone**|**Date**|**Topic Completed**|
| :-: | :-: | :-: |
|1|Nov 21|Understand Hand Gesture recognition models and generating datasets.|
|2|Nov 28|Training the CNN classifier and mapping the outputs to the background.|
|3|Dec 5|Applying segmentation to embed backgrounds(Static image/recorded videos) and Quality Assurance.|





**Organization of the team**

|**Topic**|**Worked By**|
| :-: | :-: |
|Project Idea Brainstorming, Documentation and Code quality|Everyone|
|Understanding existing hand gesture models and Generating specific datasets.|Everyone|
|Gesture Detection|<p>Ramprasad Kokkula, Pavan Kumar Jonnadula</p><p></p>|
|Training the CNN classifier and mapping output to the backgrounds.|Ramprasad Kokkula, Pavan Kumar Jonnadula|
|Segmentation of background|Aakanksha Padmanabhan, Vaishnavi Patil|
|Implementation of segmentation on the static image / live video|Aakanksha Padmanabhan, Vaishnavi Patil|

